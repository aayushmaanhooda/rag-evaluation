{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36d68433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aayushmaanhooda/Desktop/codebase/november/ragas_eval/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RAG EVALUATION WITH RAGAS\n",
    "# ============================================================================\n",
    "# This notebook demonstrates how to evaluate a RAG (Retrieval-Augmented \n",
    "# Generation) system using RAGAS metrics. It uses a PDF document about \n",
    "# \"The Ember & Oak Kitchen\" restaurant as the knowledge base.\n",
    "# ============================================================================\n",
    "\n",
    "# Load environment variables from .env file (for API keys)\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain imports for building the RAG pipeline\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# RAGAS metrics for evaluation\n",
    "from ragas.metrics import (\n",
    "    LLMContextPrecisionWithoutReference,  # Measures how relevant retrieved context is\n",
    "    LLMContextRecall,                      # Measures if all relevant info was retrieved\n",
    "    ContextEntityRecall,                   # Measures entity coverage in retrieved context\n",
    "    NoiseSensitivity,                      # Measures robustness to irrelevant context\n",
    "    ResponseRelevancy,                     # Measures relevance of generated response\n",
    "    Faithfulness                            # Measures if response is grounded in context\n",
    ")\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.metrics import LLMContextRecall\n",
    "from ragas import evaluate\n",
    "\n",
    "# Pinecone vector database for storing embeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "import os\n",
    "from datasets import Dataset  # HuggingFace datasets for RAGAS\n",
    "\n",
    "# Import test examples with ground truth answers\n",
    "from data import examples\n",
    "\n",
    "# LangChain chains for RAG pipeline\n",
    "from langchain_classic.chains import create_retrieval_chain \n",
    "from langchain_classic.chains.combine_documents.stuff import create_stuff_documents_chain \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Load environment variables (OPENAI_API_KEY, PINECONE_API_KEY)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "435c803c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index already has 33 vectors, skipping upload\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: BUILD THE RAG PIPELINE\n",
    "# ============================================================================\n",
    "# This section sets up the complete RAG pipeline including:\n",
    "# - LLM for generating responses\n",
    "# - Document loading and chunking\n",
    "# - Embeddings and vector storage in Pinecone\n",
    "# - Retriever configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize the LLM (GPT-4o) for generating responses\n",
    "# Temperature 0.6 provides a balance between creativity and consistency\n",
    "Model = init_chat_model(\"gpt-4o\", temperature = 0.6)\n",
    "\n",
    "# Load the PDF document about The Ember & Oak Kitchen\n",
    "loader = PyPDFLoader(\"The_Ember_and_Oak_Kitchen_Profile.pdf\")\n",
    "document = loader.load()\n",
    "\n",
    "# Split documents into chunks for better retrieval\n",
    "# chunk_size=1000: Each chunk is ~1000 characters\n",
    "# chunk_overlap=200: 200 characters overlap between chunks to maintain context\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(document)\n",
    "\n",
    "# Initialize OpenAI embeddings model\n",
    "# text-embedding-3-large: High-quality embeddings with 3072 dimensions\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# ============================================================================\n",
    "# PINECONE VECTOR DATABASE SETUP\n",
    "# ============================================================================\n",
    "# Set up Pinecone for storing and retrieving document embeddings\n",
    "# ============================================================================\n",
    "\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=api_key)\n",
    "index_name = \"rag-evaluation\"\n",
    "\n",
    "# Create index if it doesn't exist\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name = index_name,\n",
    "        dimension=3072,  # Must match embedding model dimension\n",
    "        metric=\"cosine\",  # Cosine similarity for semantic search\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Check if index already has documents to avoid duplicate uploads\n",
    "stats = index.describe_index_stats()\n",
    "total_vectors = stats[\"total_vector_count\"]\n",
    "\n",
    "# Only upload documents if index is empty\n",
    "if total_vectors == 0:\n",
    "    print(\"Index is empty, adding documents...\")\n",
    "    vectorstore = PineconeVectorStore.from_documents(\n",
    "        documents=all_splits,\n",
    "        embedding=embeddings,\n",
    "        index_name=index_name\n",
    "    )\n",
    "    print(f\"Added {len(all_splits)} documents to Pinecone\")\n",
    "else:\n",
    "    # Connect to existing index with documents\n",
    "    print(f\"Index already has {total_vectors} vectors, skipping upload\")\n",
    "    vectorstore = PineconeVectorStore(\n",
    "        index=index,\n",
    "        embedding=embeddings\n",
    "    )\n",
    "\n",
    "# Create retriever object that will fetch top 3 most relevant chunks for each query\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c9b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: CREATE AND TEST THE RAG CHAIN\n",
    "# ============================================================================\n",
    "# This section creates the complete RAG chain and tests it with a sample query\n",
    "# ============================================================================\n",
    "\n",
    "# Test query to verify the RAG pipeline is working\n",
    "query = \"Who founded The Ember & Oak Kitchen and what is their background?\"\n",
    "\n",
    "# Create a prompt template that instructs the LLM to answer based on retrieved context\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer based on context:\\n{context}\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create a chain that combines documents and generates answers\n",
    "document_chain = create_stuff_documents_chain(Model, prompt)\n",
    "\n",
    "# Create the complete RAG chain (retrieval + generation)\n",
    "rag_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# Test the RAG chain with a sample query\n",
    "response = rag_chain.invoke({\n",
    "    \"input\": query\n",
    "})\n",
    "\n",
    "# The response contains both the answer and the retrieved context\n",
    "# rag_answer = response[\"answer\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74c63654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 8 samples for evaluation\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: PREPARE EVALUATION DATASET\n",
    "# ============================================================================\n",
    "# This section runs the RAG pipeline on all test questions and collects:\n",
    "# - User inputs (questions)\n",
    "# - Generated responses\n",
    "# - Retrieved contexts\n",
    "# - Ground truth answers (references)\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize dictionary to store evaluation data\n",
    "evaluation_data = {\n",
    "    \"user_input\": [],          # The questions being asked\n",
    "    \"response\": [],            # RAG system's generated answers\n",
    "    \"retrieved_contexts\": [],  # Context chunks retrieved from vector DB\n",
    "    \"reference\": []            # Ground truth answers for comparison\n",
    "}\n",
    "\n",
    "# Process each example question through the RAG pipeline\n",
    "for item in examples:\n",
    "    question = item[\"question\"]\n",
    "\n",
    "    # Get response from RAG chain\n",
    "    response = rag_chain.invoke({\n",
    "        \"input\": question\n",
    "    })\n",
    "    rag_answer = response[\"answer\"]\n",
    "\n",
    "    # Get ground truth answer\n",
    "    ground_truth = item[\"ground_truth\"]\n",
    "\n",
    "    # Get retrieved contexts for this question\n",
    "    contexts = retriever.invoke(question)\n",
    "    \n",
    "    # Extract page_content from Document objects for PyArrow compatibility\n",
    "    # RAGAS requires plain text, not Document objects\n",
    "    context_texts = [doc.page_content for doc in contexts]\n",
    "\n",
    "    # Append all data to evaluation dictionary\n",
    "    evaluation_data[\"user_input\"].append(question)\n",
    "    evaluation_data[\"response\"].append(rag_answer)\n",
    "    evaluation_data[\"retrieved_contexts\"].append(context_texts)\n",
    "    evaluation_data[\"reference\"].append(ground_truth)\n",
    "\n",
    "print(f\"Collected {len(evaluation_data['user_input'])} samples for evaluation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "def715e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tarting RAGAS Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/4z_084lx0t9c6n12q0x9wf900000gn/T/ipykernel_66567/2302765178.py:16: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  evaluator_llm = LangchainLLMWrapper(Model)\n",
      "/var/folders/lk/4z_084lx0t9c6n12q0x9wf900000gn/T/ipykernel_66567/2302765178.py:17: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
      "Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  17%|█▋        | 8/48 [00:50<03:20,  5.00s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  19%|█▉        | 9/48 [00:54<03:10,  4.90s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  31%|███▏      | 15/48 [01:24<02:20,  4.26s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  50%|█████     | 24/48 [02:01<01:17,  3.21s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  60%|██████    | 29/48 [02:37<01:33,  4.94s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  65%|██████▍   | 31/48 [02:47<01:24,  4.96s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  69%|██████▉   | 33/48 [19:10<35:21, 141.40s/it]Exception raised in Job[3]: TimeoutError()\n",
      "Exception raised in Job[9]: TimeoutError()\n",
      "Exception raised in Job[15]: TimeoutError()\n",
      "Evaluating:  79%|███████▉  | 38/48 [38:18<41:10, 247.09s/it]Exception raised in Job[21]: TimeoutError()\n",
      "Evaluating:  90%|████████▉ | 43/48 [38:59<06:15, 75.15s/it] Exception raised in Job[27]: TimeoutError()\n",
      "Evaluating:  92%|█████████▏| 44/48 [39:02<03:40, 55.17s/it]Exception raised in Job[33]: TimeoutError()\n",
      "Evaluating:  96%|█████████▌| 46/48 [39:27<01:07, 33.68s/it]Exception raised in Job[39]: TimeoutError()\n",
      "Evaluating:  98%|█████████▊| 47/48 [39:44<00:28, 28.78s/it]Exception raised in Job[41]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 48/48 [39:59<00:00, 49.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAGAS Results:\n",
      "{'llm_context_precision_without_reference': 0.7917, 'context_recall': 0.8333, 'context_entity_recall': 0.2677, 'noise_sensitivity(mode=relevant)': 0.0000, 'answer_relevancy': 0.8086, 'faithfulness': 0.9041}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: EVALUATE WITH RAGAS METRICS\n",
    "# ============================================================================\n",
    "# This section evaluates the RAG system using multiple RAGAS metrics to\n",
    "# assess retrieval quality, response relevance, and faithfulness\n",
    "# ============================================================================\n",
    "\n",
    "# Convert evaluation data to HuggingFace Dataset format\n",
    "# RAGAS evaluate() function requires Dataset format, not a regular Python dict\n",
    "dataset = Dataset.from_dict(evaluation_data)\n",
    "\n",
    "# Use a smaller, faster embedding model for evaluation metrics\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Wrap LangChain components for RAGAS compatibility\n",
    "evaluator_llm = LangchainLLMWrapper(Model)\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "# Define evaluation metrics\n",
    "# Each metric measures a different aspect of RAG performance:\n",
    "metrics = [\n",
    "    LLMContextPrecisionWithoutReference(),  # How precise is the retrieved context?\n",
    "    LLMContextRecall(),                      # Did we retrieve all relevant information?\n",
    "    ContextEntityRecall(),                   # Are key entities in the retrieved context?\n",
    "    NoiseSensitivity(),                      # Is the system robust to irrelevant context?\n",
    "    ResponseRelevancy(),                     # Is the response relevant to the question?\n",
    "    Faithfulness(),                          # Is the response faithful to the context?\n",
    "]\n",
    "\n",
    "print(\"\\ntarting RAGAS Evaluation...\")\n",
    "# Run evaluation on the dataset\n",
    "# This will take several minutes as it processes each question with each metric\n",
    "result = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=metrics,\n",
    "    llm=evaluator_llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "print(\"\\nRAGAS Results:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479351a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
